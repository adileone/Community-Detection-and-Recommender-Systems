{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd  \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client['twitter']\n",
    "\n",
    "pipeline=[{ \"$project\": {  \"id_user\" : 1, \"originalTweet\" : 1, \"hashtag\" : 1, \"mention\" : 1} }]\n",
    "\n",
    "cursor_list = list(db['tweets'].aggregate(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(cursor_list)\n",
    "data = pd.DataFrame(columns=['Tweets'])\n",
    "data['Tweets'] = tweets['originalTweet']\n",
    "data['user_id'] = tweets['id_user']\n",
    "data['len']  = np.array([len(tweet) for tweet in data['Tweets']])\n",
    "data['hahtags'] = tweets['hashtag']\n",
    "data['mentions'] = tweets['mention']\n",
    "# user = list()\n",
    "# for i in range(0,len(tweets)):\n",
    "#     user.append(tweets['user'][i]['name'])\n",
    "# data['user'] = np.array(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from nltk.corpus import stopwords\n",
    "from utils import tokenize_and_stem\n",
    " \n",
    "    \n",
    "documents = data['Tweets']\n",
    "\n",
    "myStopWords=set(stopwords.words('english')+list(['http','https']))                             \n",
    "    \n",
    "vectorizer = TfidfVectorizer(use_idf=True,\n",
    "                             ngram_range=(1,5),\n",
    "                             min_df=100, max_features=300, tokenizer=tokenize_and_stem, stop_words=myStopWords, )\n",
    "\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster : 0\n",
      "['cnnbrk', 'says', 'police', 'killed', 'dead']\n",
      "Cluster : 1\n",
      "['much', 'thank', 'love', 'like', 'thanks']\n",
      "Cluster : 2\n",
      "['take', 'look', 'care', 'time', 'trump']\n",
      "Cluster : 3\n",
      "['trump', 'like', 'today', 'time', 'people']\n",
      "Cluster : 4\n",
      "['find', 'help', 'people', 'need', 'could']\n",
      "Cluster : 5\n",
      "['really', 'like', 'good', 'want', 'people']\n",
      "Cluster : 6\n",
      "['bill', 'clinton', 'trump', 'hillary', 'house']\n",
      "Cluster : 7\n",
      "['work', 'hard', 'today', 'great', 'good']\n",
      "Cluster : 8\n",
      "['times', 'york', 'israel', 'trump', 'many']\n",
      "Cluster : 9\n",
      "['says', 'trump', 'breaking', 'clinton', 'reuters']\n"
     ]
    }
   ],
   "source": [
    "true_k = 10\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster :\", i),\n",
    "    lista = [terms[ind] for ind in order_centroids[i, :5]]\n",
    "    print(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 4615815, 9: 65357, 1: 59220, 2: 39535, 7: 36880, 5: 36826, 8: 24569, 4: 23669, 6: 19144, 0: 13812})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "counter=collections.Counter(model.labels_)\n",
    "print (counter)\n",
    "print ('Silouette_score: ',silhouette_score(X, model.labels_))\n",
    "\n",
    "plt.bar(range(len(set(model.labels_))), np.bincount(model.labels_))\n",
    "\n",
    "plt.ylabel('population')\n",
    "plt.xlabel('cluster label')\n",
    "plt.title('population sizes with {} clusters'.format(true_k));\n",
    "\n",
    "# truncate y axis to see the rest better\n",
    "plt.ylim(0,counter[0]+1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=1).fit(X)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
